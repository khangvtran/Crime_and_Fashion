---
title: "Crime and Communities"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---


**Group Member 1 Name: **Khang V. Tran  **Group Member 1 SID: **25181590

**Group Member 2 Name: **Christian Philip Hoeck   **Group Member 2 SID: **3035385003

The crime and communities dataset contains crime data from communities in the United States. The data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR. More details can be found at https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized.

The dataset contains 125 columns total; $p=124$ predictive and 1 target (ViolentCrimesPerPop). There are $n=1994$ observations. These can be arranged into an $n \times p = 1994 \times 127$ feature matrix $\mathbf{X}$, and an $n\times 1 = 1994 \times 1$ response vector $\mathbf{y}$ (containing the observations of ViolentCrimesPerPop).

Once downloaded (from bCourses), the data can be loaded as follows.

```{r, message=FALSE, warning = FALSE}
library(readr)
CC <- read_csv("../data_files/crime_and_communities_data.csv")
print(dim(CC))
y <- CC$ViolentCrimesPerPop
X <- subset(CC, select = -c(ViolentCrimesPerPop))
```


# Part 1) Dataset Exploration - Feature Creating and Engineering

In this section, you should provide a thorough exploration of the features of the dataset. Things to keep in mind in this section include:

- Which variables are categorical versus numerical?
- What are the general summary statistics of the data? How can these be visualized?
- Is the data normalized? Should it be normalized?
- Are there missing values in the data? How should these missing values be handled? 
- Can the data be well-represented in fewer dimensions?

**YOUR CODE GOES HERE**

\newpage

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(dplyr)
```


## Missing Data Processing

check if target y contains missing data
```{r}
any(is.na(y))
```

check if any of the features contains missing data
```{r}
any(is.na(X))
```

Now, as we have detected that there exist feature(s) in X that contain NA, the next step is to find and remove features with high proportion of NA (50% or above) and remove them all together before process those that have an acceptable number of NA
```{r}

get_na_perc <- function(x, n){
  return(sum(is.na(x))/n)
}

get_na_perc_all_features <- function(features){
  n = nrow(features)
  perc <- apply(X = features, MARGIN = 2, FUN = function(x) sum(is.na(x))/n)
  return(perc)
}

get_feature_high_na <- function(features, threshold){
  perc <- get_na_perc_all_features(features)
  return(names(perc[perc >= threshold]))
}

feature_high_na <- get_feature_high_na(X, .5)
print(feature_high_na)
X <- X %>% select(-feature_high_na)
```


Now that we have only feature with none or a low number of NA left, let's replace the mising values with the median
```{r}
X <- X %>% mutate_all(function(x) ifelse(is.na(x), median(x, na.rm = TRUE), x))
any(is.na(X))
```

\newpage

## Examine Categorical vs. Quantitative data

```{r, results="hide"}
str(X)
```

By examine the structure of the data using str() (result not shown due to exessive printing) we can explore the datatype of each feature. So far, we are be able to see that there is no factor type, which means there is no string categorical feature. Neither str() nor apply(class) shows any factor. Just to be certain, I examine the documentation from the source (UC Irvine): https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized and did not find any character nor factor class. However, more examination is needed since there might be numerical catergorical data.


```{r}
hist(X$householdsize)
print(head(X$householdsize, n = 20))
```

By plotting histogram, we can pick out some categorical-likely feature. One of them is $householdsize$. However, when verifying with the documentation, we see that $householdsize$ is actually  the mean people per household (numeric - decimal). By printing out the first few values, we are able to confirm this. Therefore, it is not categorical



```{r}
hist(X$MedNumBR)
print(head(X$MedNumBR))
```

The next candidate is $MedNumBR$. This is, indeed, categorical and we can confirm this by once again using histogram and by printing out the first few values. However, since this is numerical, it does not matter if this is categorical. We leave it as is.

There exist in the original data the feature of states, county code, and community code, which are catergorical. However, they are not included in the given data. On the other hads, all other quantitative features in the original data are. We can say that the data set is entirely quantitative.

\newpage

## Summary Statistics 

When speaking about crimes, there are factors that need to be taken into consideration. They are Unemployment, Children of Single Parent, Homelessness/Rent, and Poverty. Due to the high number of feature, we will only select a few feature that will give a very general idea about some of these factors

```{r}
interesting_features <- c("PctUnemployed", "NumKidsBornNeverMar", "MedRentPctHousInc", "PctPopUnderPov")

summary_stat <- function(features, selection){
  stats <- apply(X = features[, selection], MARGIN = 2, FUN = summary)
  return(stats)
}

summary_stat(X, interesting_features)
```


\newpage

## Visualization

Due to such as massive number of feature, there is no way to visualize data from every feature without dimentionality reduction. In the next coming graphs, we only examine some groups of feature that will hopefully tell us something about the data.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
```


```{r, echo = FALSE}
temp_df <- X %>% select(matches("Empl|Par|Pov")) %>% mutate(CrimeRate = y)
# print(head(temp_df))
```


```{r}
ggplot(data = temp_df) + 
  geom_point(aes(x = PctUnemployed, color = CrimeRate, y = 1/PctKids2Par)) +
  scale_color_gradient(low = "pink", high = "black") + 
  xlab("Unemployment Rate") +
  ylab("Single Parent Househole Rate") +
  ggtitle("Crime Rate with respect to Unemployment rate and Single Parent Family Rate")
```

\newpage

```{r}
ggplot(data = temp_df) +
  geom_point(aes(x = PctPopUnderPov, y = CrimeRate, alpha = 0.3)) + 
  xlab("Percentage under Poverty") +
  ggtitle("Crime Rate with respect to Poverty")
```

As we can see, there is a correlation between crime and poverty as well as crime and unemployment. However, these relartionship are not exactly linear.

\newpage


## Data Normalization - Scaling

After the previous step of examination, it is obvious that many features are different in nature. For example, some feautures are Percentage (PctForeignBorn, PctBornSameState). Some are counts (NumInShelters, population). Some are in US Dollars (MedRent, ...). Each of the features have different range, scale, and unit. Such condition will affect how much each of the feature influence the predition later on .Therefore, it is highly crucial that we normalize the features.

```{r}
X <- scale(X)
```

## Dimensionality reduction - Principal Component Analysis

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(FactoMineR)
```

Apply PCA
```{r}
res.pca <- PCA(X = X,graph = F,ncp = 30)
```

Plot Screeplot for Eigenvalues. Due to the very high number of components (125), we only pick out the first 20

```{r}
eig <- res.pca$eig
```

Visualize Eigenvalue
```{r}
eigvalue <- eig[1:15, "eigenvalue"]

barchart <- barplot(eigvalue, las = 1, border = NA,
                    names.arg = 1:length(eigvalue),
                    ylim = c(0, 1.1 * ceiling(max(eigvalue))), 
                    ylab = "value",
                    xlab = "Eigenvalues - how much variance the corresponding PC captures", 
                    main = "Scree plot")

points(barchart, eigvalue, pch = 19, col = "deepskyblue4") 
lines(barchart, eigvalue, lwd = 2, col = "deepskyblue")
```

As you can see, each of the eigencalue represents the amount of variance in the dataset that was captured by the corresponding PC. Also, let's examine the eigen value result overall

```{r}
head(eig, n = 10)
```

\newpage

With the given information above we can choose the number of component based on:

- Elbow method: the first 6 PCs

- Kaiser's rule $\lambda_k > 1$: the first 20 PCs

- Jollie's rule $\lambda_k > 0.7$: the first 30 PCs

- A, if we wish to keep the number of PCs that accumulatively capture 70% of the variance in the data, we can keep the first 10 PCs



```{r, echo = FALSE}
p <- PCA(X[, 1:10], graph = F)
# p <- PCA(X, graph = F)
plot(p,choix="var",new.plot=FALSE)
```


```{r}
# names(PCs) # "coord"   "cor"     "cos2"    "contrib"
PCs <- res.pca$ind$coord
print(head(PCs, 3))
```



\newpage

# Part 2) Regression task

In this section, you should use the techniques learned in class to develop a model to predict ViolentCrimesPerPop using the 124 features (or some subset of them) stored in $\mathbf{X}$. Remember that you should try several different methods, and use model selection methods to determine which model is best. You should also be sure to keep a held-out test set to evaluate the performance of your model.

**YOUR CODE GOES HERE**

## Part 2.1) Train - Test - Validation Data Randomization

In order to perform hyperparameter tuning within a model and model selecting, we will apply three-way hold-out method. Meaning, the data will be randomized into:

- Train: 60% of the data
- Validation: 20% of the data
- Test: 20% of the data

We will then performed training, and hyperparameter selection

```{r}
n <- nrow(X)
indices <- 1:n

# randomize 60% of the original data to be the train set
train_indices <- sample(x = indices, size = round(.6*n))
train_pcs <- PCs[train_indices, ]
train_features <- X[train_indices, ]
train_target <- y[train_indices]

# randomize 20% of the original data to be the validation test
# (50% of the remaining data after sampling the train set)
validation_indices <- sample(x = indices[-train_indices],
                  size = round(.5*length(indices[-train_indices])))
validation_pcs <- PCs[validation_indices, ]
validation_features <- X[validation_indices, ]
validation_target <- y[validation_indices]

# use the rest of the data (20% of the original dataset) to be the final dataset
test_indices <- sample(x = indices[c(-train_indices,
                                     -validation_indices)])
validation_pcs <- PCs[validation_indices, ]
validation_features <- X[validation_indices, ]
validation_target <- y[validation_indices]

```

\newpage

## Part 2.2) Training Phase
 
## Part 2.3) Hyper Parameter Tuning
 
## Part 2.4) Model Selection
 
## Part 2.5) Final Model


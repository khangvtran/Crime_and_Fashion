---
title: "Crime and Communities"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---


**Group Member 1 Name: **Khang V. Tran  **Group Member 1 SID: **25181590

**Group Member 2 Name: **Christian Philip Hoeck   **Group Member 2 SID: **_______________

The crime and communities dataset contains crime data from communities in the United States. The data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR. More details can be found at https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized.

The dataset contains 125 columns total; $p=124$ predictive and 1 target (ViolentCrimesPerPop). There are $n=1994$ observations. These can be arranged into an $n \times p = 1994 \times 127$ feature matrix $\mathbf{X}$, and an $n\times 1 = 1994 \times 1$ response vector $\mathbf{y}$ (containing the observations of ViolentCrimesPerPop).

Once downloaded (from bCourses), the data can be loaded as follows.

```{r, message=FALSE, warning = FALSE}
library(readr)
CC <- read_csv("../data_files/crime_and_communities_data.csv")
print(dim(CC))
y <- CC$ViolentCrimesPerPop
X <- subset(CC, select = -c(ViolentCrimesPerPop))
```


# Dataset exploration

In this section, you should provide a thorough exploration of the features of the dataset. Things to keep in mind in this section include:

- Which variables are categorical versus numerical?
- What are the general summary statistics of the data? How can these be visualized?
- Is the data normalized? Should it be normalized?
- Are there missing values in the data? How should these missing values be handled? 
- Can the data be well-represented in fewer dimensions?

**YOUR CODE GOES HERE**

\newpage

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(dplyr)
```

## Examine Categorical vs. Quantitative data
Let's look at the structure of the data
```{r}
str(X)
```

\newpage

The structure of the data is partialy ommited due to the high number of features. Let's try getting the class of each feature
```{r}
apply(X = X, MARGIN = 2, FUN = class)
```

Neither str() nor apply(class) shows any factor. Just to be certain, I examine the documentation from the source (UC Irvine): https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized

There exist in the original data the feature of states, county code, and community code, which are catergorical. However, they are not included in the given data. On the other hads, all other quantitative features in the original data are. We can say that the data set is entirely quantitative.

\newpage

## Summary Statistics 
#########################################
How many do I show?
```{r}
summary(X)
```


## Visualization

Due to such as massive number of feature, there is no way to visualize data from every feature without dimentionality reduction. In the next coming graphs, we only examine some groups of feature that will hopefully tell us something about the data.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
```


```{r, echo = FALSE}
employment_parents <- X %>% select(matches("Empl|Par")) %>% mutate(CrimeRate = y)

# ggplot(data = employment_parents) + 
#   geom_point(aes(x = PctUnemployed, y = CrimeRate, color = PctKids2Par)) +
#   scale_color_gradient(low = "red", high = "white") + 
#   ggtitle("Crime Rate with respect to Unemployment rate and Single Parent Family Rate")

ggplot(data = employment_parents) + 
  geom_point(aes(x = PctUnemployed, color = CrimeRate, y = 1/PctKids2Par)) +
  scale_color_gradient(low = "pink", high = "black") + 
  xlab("Unemployment Rate") +
  ylab("Single Parent Househole Rate") +
  ggtitle("Crime Rate with respect to Unemployment rate and Single Parent Family Rate")

```


\newpage

## Missing Data Processing

check if target y contains missing data
```{r}
any(is.na(y))
```

check if any of the features contains missing data
```{r}
any(is.na(X))
```

Now that we have detected there is NA in some the features, we decide to replace it by the median of other existing data in that corresponding feature
```{r}
X <- X %>% mutate_all(function(x) ifelse(is.na(x), median(x, na.rm = TRUE), x))
any(is.na(X))
```

## Data Normalization - Scaling

After the previous step of examination, it is obvious that many features are different in nature. For example, some feautures are Percentage (PctForeignBorn, PctBornSameState). Some are counts (NumInShelters, population). Some are in US Dollars (MedRent, ...). Each of the features have different range, scale, and unit. Such condition will affect how much each of the feature influence the predition later on .Therefore, it is highly crucial that we normalize the features.

```{r}
# X <- scale(X)
```

## Dimensionality reduction - Principal Component Analysis

```{r, echo = FALSE, warning=FALSE, message=FALSE}
library(FactoMineR)
```

Apply PCA
```{r}
res.pca <- PCA(X = X,graph = F,ncp = 10)
```

Plot Screeplot for Eigenvalues. Due to the very high number of components (125), we only pick out the first 20

```{r}
eig <- res.pca$eig
```

Visualize Eigen value
```{r}
eigvalue <- eig[1:15, "eigenvalue"]

barchart <- barplot(eigvalue, las = 1, border = NA,
                    names.arg = 1:length(eigvalue),
                    ylim = c(0, 1.1 * ceiling(max(eigvalue))), 
                    ylab = "value",
                    xlab = "Eigenvalues - how much variance the corresponding PC captures", 
                    main = "Scree plot")

points(barchart, eigvalue, pch = 19, col = "deepskyblue4") 
lines(barchart, eigvalue, lwd = 2, col = "deepskyblue")
```
As you can see, each of the eigencalue represents the amount of variance in the dataset that was captured by the corresponding PC. Also, let's examine the eigen value result overall

```{r}
head(eig, n = 35)
```

With the given information above we can choose the number of component based on:

- Elbow method: the first 6 PCs

- Kaiser's rule $\lambda_k > 1$: the first 20 PCs

- Jollie's rule $\lambda_k > 0.7$: the first 30 PCs

- A, if we wish to keep the number of PCs that accumulatively capture 70% of the variance in the data, we can keep the first 10 PCs


#########################################
Interpretation of this plot
```{r}
p <- PCA(X[, 1:20], graph = T)
```

#########################################
Question for Ryan: 
Now my plan is that:
Keep 10 PCs only and perform regression on PCs instead of doing so on the original data matrix. Which one of the ] "coord", "cor", "cos2" ,"contrib" should I use?

Also, do I scale principal component again


```{r}
# head(res.pca$var)
PCs <- res.pca$var
names(PCs) # "coord"   "cor"     "cos2"    "contrib"
PCs <- res.pca$var[["coord"]]
```

# Regression task

In this section, you should use the techniques learned in class to develop a model to predict ViolentCrimesPerPop using the 124 features (or some subset of them) stored in $\mathbf{X}$. Remember that you should try several different methods, and use model selection methods to determine which model is best. You should also be sure to keep a held-out test set to evaluate the performance of your model.



**YOUR CODE GOES HERE**